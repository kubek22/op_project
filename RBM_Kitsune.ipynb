{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhQUQP5uo9DJ",
        "outputId": "93e276d6-5801-49a2-a3c9-756da8e99250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = '/content/drive/MyDrive/kitsune+network+attack+dataset.zip'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "extract_dir = '/content/kitsune'"
      ],
      "metadata": {
        "id": "gufcL50MJKpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "extract_dir = '/content/kitsune'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(DATA_DIR, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "xsa3kUEdq74Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_dirs = sorted([d for d in os.listdir(extract_dir) if os.path.isdir(os.path.join(extract_dir, d))])\n",
        "print(\"Available attack directories:\")\n",
        "for attack in attack_dirs:\n",
        "    print(\" -\", attack)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvR4ouGmJUHZ",
        "outputId": "9bf10ee1-b298-4b8f-ac1f-64de23759056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available attack directories:\n",
            " - active_wiretap\n",
            " - arp_mitm\n",
            " - fuzzing\n",
            " - mirai\n",
            " - os_scan\n",
            " - ssdp_flood\n",
            " - ssl_renegotiation\n",
            " - syn_dos\n",
            " - video_injection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_sample = 0.1\n",
        "\n",
        "\n",
        "attack_mapping = {\n",
        "    'os_scan'            : 'OS Scan',\n",
        "    'fuzzing'            : 'Fuzzing',\n",
        "    'video_injection'    : 'Video Injection',\n",
        "    'arp_mitm'           : 'ARP MitM',\n",
        "    'active_wiretap'     : 'Active Wiretap',\n",
        "    'ssdp_flood'         : 'SSDP Flood',\n",
        "    'syn_dos'            : 'SYN DoS',\n",
        "    'ssl_renegotiation'  : 'SSL Renegotiation',\n",
        "    'mirai'              : 'Mirai'\n",
        "}\n",
        "\n",
        "# features_dfs = {}\n",
        "# labels_dfs   = {}\n",
        "errors       = []\n",
        "\n",
        "\n",
        "feature_list = []\n",
        "label_list   = []\n",
        "\n",
        "# ─── 4) Loop over each directory, build exact filenames, and load ────────────────\n",
        "\n",
        "for folder in attack_dirs:\n",
        "    if folder not in attack_mapping:\n",
        "        errors.append(f\"→ No mapping found for folder: '{folder}'\")\n",
        "        continue\n",
        "\n",
        "    base_name   = attack_mapping[folder]          # e.g. \"Active Wiretap\"\n",
        "    attack_path = os.path.join(extract_dir, folder)\n",
        "\n",
        "    # Build the expected .csv.gz paths\n",
        "    features_path = os.path.join(attack_path, f\"{base_name}_dataset.csv.gz\")\n",
        "    labels_path   = os.path.join(attack_path, f\"{base_name}_labels.csv.gz\")\n",
        "\n",
        "    try:\n",
        "        # --- 4a) Load features ---\n",
        "        print(f\"Loading features for '{folder}' from:\\n  {features_path}\")\n",
        "        df_feat = pd.read_csv(features_path, compression='gzip', header=None)\n",
        "        print(f\"  → Features shape: {df_feat.shape}\")\n",
        "\n",
        "\n",
        "        if folder == 'mirai':\n",
        "            # Mirai labels file has only one column (no index column)\n",
        "            print(f\"Loading MIRAI labels (single‐column) from:\\n  {labels_path}\")\n",
        "            df_lbl = pd.read_csv(\n",
        "                labels_path,\n",
        "                compression='gzip',\n",
        "                header=None,     # assume first line is a header like \"label\", drop it\n",
        "            )\n",
        "        else:\n",
        "            # --- 4b) Load labels (parse index,col properly) ---\n",
        "            print(f\"Loading labels for '{folder}' from:\\n  {labels_path}\")\n",
        "            df_lbl = pd.read_csv(\n",
        "              labels_path,\n",
        "              compression='gzip',\n",
        "              sep=',',           # split “index,label”\n",
        "              header=0,          # treat first line (e.g. \"0,x\") as header\n",
        "              index_col=0,       # drop the index‐column\n",
        "              names=['index','label']\n",
        "            )\n",
        "            print(f\"  → Labels shape:   {df_lbl.shape}\")\n",
        "            df_lbl = df_lbl['label']\n",
        "\n",
        "        # 4c) Verify row counts match\n",
        "        if df_feat.shape[0] != df_lbl.shape[0]:\n",
        "            msg = (\n",
        "                f\"features {df_feat.shape[0]} vs labels {df_lbl.shape[0]}\"\n",
        "            )\n",
        "            errors.append(msg)\n",
        "        else:\n",
        "            print(f\"Row counts match ({df_feat.shape[0]} packets)\\n\")\n",
        "\n",
        "\n",
        "        # sample k_sample from entire dataset (stratified on y)\n",
        "        _, df_feat_sample, _, df_lbl_sample = train_test_split(\n",
        "                    df_feat,\n",
        "                    df_lbl,\n",
        "                    test_size=k_sample,\n",
        "                    random_state=0,\n",
        "                    stratify=df_lbl\n",
        "                )\n",
        "\n",
        "        del df_feat, df_lbl, _\n",
        "        # Reset indices for consistency\n",
        "        df_feat_sample = df_feat_sample.reset_index(drop=True)\n",
        "        df_lbl_sample  = df_lbl_sample.reset_index(drop=True)\n",
        "\n",
        "        feature_list.append(df_feat_sample)\n",
        "        label_list.append(df_lbl_sample)\n",
        "\n",
        "        del df_feat_sample, df_lbl_sample\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Failed to load '{folder}': {e}\\n\")\n",
        "\n",
        "# ─── 5) Summary ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"────────────────────────────────────────\")\n",
        "print(f\"Successfully loaded {len(feature_list)} attack datasets.\")\n",
        "if errors:\n",
        "    print(\"\\nSome folders could not be loaded or had mismatches:\")\n",
        "    for e in errors:\n",
        "        print(\" \", e)\n",
        "else:\n",
        "    print(\"No errors detected. All row counts match.\\n\")\n",
        "\n",
        "# Now you can access, for example:\n",
        "#   features_dfs['active_wiretap']   → pandas DataFrame of shape (N, 115)\n",
        "#   labels_dfs  ['active_wiretap']   → pandas Series of length N\n",
        "\n",
        "# Example: display the first 3 rows of “active_wiretap” features & labels\n",
        "# demo_folder = 'active_wiretap'\n",
        "# if demo_folder in features_dfs:\n",
        "#     print(f\"\\nExample: first 3 rows of {demo_folder} features:\")\n",
        "#     display(features_dfs[demo_folder].head(3))\n",
        "#     print(f\"\\nExample: first 10 labels of {demo_folder}:\")\n",
        "#     display(labels_dfs[demo_folder].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3US3BqUyZbn",
        "outputId": "de7ee382-a3c8-494c-dd2d-bd02374c04ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading features for 'active_wiretap' from:\n",
            "  /content/kitsune/active_wiretap/Active Wiretap_dataset.csv.gz\n",
            "  → Features shape: (2278689, 115)\n",
            "Loading labels for 'active_wiretap' from:\n",
            "  /content/kitsune/active_wiretap/Active Wiretap_labels.csv.gz\n",
            "  → Labels shape:   (2278689, 1)\n",
            "Row counts match (2278689 packets)\n",
            "\n",
            "Loading features for 'arp_mitm' from:\n",
            "  /content/kitsune/arp_mitm/ARP MitM_dataset.csv.gz\n",
            "  → Features shape: (2504267, 115)\n",
            "Loading labels for 'arp_mitm' from:\n",
            "  /content/kitsune/arp_mitm/ARP MitM_labels.csv.gz\n",
            "  → Labels shape:   (2504267, 1)\n",
            "Row counts match (2504267 packets)\n",
            "\n",
            "Loading features for 'fuzzing' from:\n",
            "  /content/kitsune/fuzzing/Fuzzing_dataset.csv.gz\n",
            "  → Features shape: (2244139, 115)\n",
            "Loading labels for 'fuzzing' from:\n",
            "  /content/kitsune/fuzzing/Fuzzing_labels.csv.gz\n",
            "  → Labels shape:   (2244139, 1)\n",
            "Row counts match (2244139 packets)\n",
            "\n",
            "Loading features for 'mirai' from:\n",
            "  /content/kitsune/mirai/Mirai_dataset.csv.gz\n",
            "  → Features shape: (764137, 116)\n",
            "Loading MIRAI labels (single‐column) from:\n",
            "  /content/kitsune/mirai/Mirai_labels.csv.gz\n",
            "Row counts match (764137 packets)\n",
            "\n",
            "Loading features for 'os_scan' from:\n",
            "  /content/kitsune/os_scan/OS Scan_dataset.csv.gz\n",
            "  → Features shape: (1697851, 115)\n",
            "Loading labels for 'os_scan' from:\n",
            "  /content/kitsune/os_scan/OS Scan_labels.csv.gz\n",
            "  → Labels shape:   (1697851, 1)\n",
            "Row counts match (1697851 packets)\n",
            "\n",
            "Loading features for 'ssdp_flood' from:\n",
            "  /content/kitsune/ssdp_flood/SSDP Flood_dataset.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(feature_list))\n",
        "print(feature_list[0].shape)"
      ],
      "metadata": {
        "id": "O2bQGcVJkAhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_list[3] = feature_list[3].iloc[:, 1:]\n",
        "label_list   = []\n",
        "\n",
        "for folder in list(features_dfs.keys()):\n",
        "    df_feat = features_dfs.pop(folder)\n",
        "    df_lbl  = labels_dfs.pop(folder)\n",
        "\n",
        "    if folder == 'mirai':\n",
        "        df_feat = df_feat.iloc[:, 1:]\n",
        "\n",
        "    feature_list.append(df_feat)\n",
        "    label_list.append(df_lbl)"
      ],
      "metadata": {
        "id": "OzCD95qiBehs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.concat(feature_list, ignore_index=True)\n",
        "y = pd.concat(label_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "-8xuJnTZJYkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free memory\n",
        "for name in dir():\n",
        "    if name not in ['X', 'y'] and not name.startswith('_'):\n",
        "        del globals()[name]\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0QxgVWUWJZrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, X_val, y, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X.shape}\")\n",
        "print(f\"X_val   shape: {X_val.shape}\")\n",
        "print(f\"y_train shape: {y.shape}\")\n",
        "print(f\"y_val   shape: {y_val.shape}\")"
      ],
      "metadata": {
        "id": "fErFu3ztBSw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import torch\n",
        "# from torch_rbm import RBM as TRBM\n",
        "import datetime\n",
        "#from softmax import Net, train\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "CdY1AvdgBqpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input, output=2):\n",
        "        super().__init__()\n",
        "        self.soft = nn.Linear(input, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.soft(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def training_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    n = 0\n",
        "    for x, y in dataloader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        correct += predicted.eq(y).sum().item()\n",
        "        n += y.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / n\n",
        "    accuracy = 100 * correct / n\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "\n",
        "def evaluate(model,dataloader, criterion, device):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    n = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            output = model(x)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "\n",
        "            loss = criterion(output, y)\n",
        "            total_loss += loss.item()\n",
        "            n += y.size(0)\n",
        "            correct += predicted.eq(y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / n\n",
        "    accuracy = 100 * correct / n\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "\n",
        "def train(epochs, model, dataloader_train, dataloader_val, optimizer, criterion, device, model_path,\n",
        "        tolerance=math.inf):\n",
        "    train_accuracy_list, train_loss_list = [], []\n",
        "    val_accuracy_list, val_loss_list = [], []\n",
        "    best_loss = float('inf')\n",
        "    last_save = 0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_accuracy, train_avg_loss = training_epoch(model, dataloader_train, optimizer, criterion, device)\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "        train_loss_list.append(train_avg_loss)\n",
        "        print(f\"epoch: {epoch + 1}, training loss: {train_avg_loss}, training accuracy: {train_accuracy}\")\n",
        "\n",
        "        val_accuracy, val_avg_loss = evaluate(model, dataloader_val, criterion, device)\n",
        "        val_accuracy_list.append(val_accuracy)\n",
        "        val_loss_list.append(val_avg_loss)\n",
        "        print(f\"epoch: {epoch + 1}, validation loss: {val_avg_loss}, validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        if val_avg_loss < best_loss:\n",
        "            best_loss = val_avg_loss\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            last_save = epoch + 1\n",
        "            epochs_without_improvement = 0\n",
        "            print(\"model saved\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement > tolerance:\n",
        "                print(f\"Training stopped. Tolerance {tolerance} exceeded\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "    history = {\n",
        "        \"loss_train\": train_loss_list,\n",
        "        \"accuracy_train\": train_accuracy_list,\n",
        "        \"loss_val\": val_loss_list,\n",
        "        \"accuracy_val\": val_accuracy_list,\n",
        "        \"last_save\": last_save\n",
        "    }\n",
        "    return history"
      ],
      "metadata": {
        "id": "rwwWLWu8B6yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden, device='cpu'):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        self.a = torch.zeros(self.n_visible).to(self.device).double()\n",
        "        self.b = torch.zeros(self.n_hidden).to(self.device).double()\n",
        "        self.W = torch.zeros((self.n_visible, self.n_hidden)).to(self.device).double()\n",
        "\n",
        "    def state_dict(self):\n",
        "        state_dict = {\n",
        "            \"n_visible\": self.n_visible,\n",
        "            \"n_hidden\": self.n_hidden,\n",
        "            \"device\": self.device,\n",
        "            \"a\": self.a,\n",
        "            \"b\": self.b,\n",
        "            \"W\": self.W\n",
        "        }\n",
        "        return state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.n_visible = state_dict[\"n_visible\"]\n",
        "        self.n_hidden = state_dict[\"n_hidden\"]\n",
        "        self.device = state_dict[\"device\"]\n",
        "        self.a = state_dict[\"a\"]\n",
        "        self.b = state_dict[\"b\"]\n",
        "        self.W = state_dict[\"W\"]\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        self.a = self.a.to(device)\n",
        "        self.b = self.b.to(device)\n",
        "        self.W = self.W.to(device)\n",
        "        return self\n",
        "\n",
        "    def h_probability(self, v):\n",
        "        v = v.to(self.device)\n",
        "        return torch.sigmoid(self.b + v @ self.W)\n",
        "\n",
        "    def v_probability(self, h):\n",
        "        h = h.to(self.device)\n",
        "        return torch.sigmoid(self.a + (self.W @ h.T).T)\n",
        "\n",
        "    def draw_hidden(self, v):\n",
        "        v = v.to(self.device)\n",
        "        p = self.h_probability(v)\n",
        "        h = torch.bernoulli(p)\n",
        "        return h\n",
        "\n",
        "    def draw_visible(self, h):\n",
        "        h = h.to(self.device)\n",
        "        # v = torch.zeros(self.n_visible).to(self.device)\n",
        "        p = self.v_probability(h) # keep tensor shape\n",
        "        v = torch.bernoulli(p)\n",
        "        return v\n",
        "\n",
        "    def gibbs_sampling(self, n, h):\n",
        "        h = h.to(self.device)\n",
        "        for i in range(n):\n",
        "            v = self.draw_visible(h)\n",
        "            h = self.draw_hidden(v)\n",
        "        return v, h\n",
        "\n",
        "    def fit(self, V, iterations, learning_rate, cd_n=1, batch_size=64, verbose=False):\n",
        "        dataset = torch.utils.data.TensorDataset(V)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for i in range(iterations):\n",
        "            if verbose:\n",
        "                print(f\"Iteration: {i + 1} of {iterations}\")\n",
        "            for batch in dataloader:\n",
        "                v = batch[0].to(self.device)\n",
        "                h = self.draw_hidden(v)\n",
        "                v_cd, h_cd = self.gibbs_sampling(cd_n, h)\n",
        "\n",
        "                self.W += learning_rate * ((v.T @ h - v_cd.T @ h_cd) / v.size(0))\n",
        "                self.a += learning_rate * torch.mean(v - v_cd, dim=0)\n",
        "                self.b += learning_rate * torch.mean(h - h_cd, dim=0)\n"
      ],
      "metadata": {
        "id": "V1Z03-RMCBOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRBM = RBM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "wvtrEEBURke4",
        "outputId": "a1942e38-fd4a-4313-c503-90c3e9155bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RBM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-382d4e0cf333>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTRBM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'RBM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standard_scaler = StandardScaler()\n",
        "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "X_stand = standard_scaler.fit_transform(X)\n",
        "X_norm = minmax_scaler.fit_transform(X_stand)\n",
        "\n",
        "X_val_stand = standard_scaler.transform(X_val)\n",
        "X_val_norm = minmax_scaler.transform(X_val_stand)\n",
        "del X, X_val, X_stand, X_val_stand"
      ],
      "metadata": {
        "id": "kYmdwUiDH6Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_norm, dtype=torch.double)\n",
        "Y_train_tensor = torch.tensor(np.array(y), dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val_norm, dtype=torch.double)\n",
        "Y_val_tensor = torch.tensor(np.array(y_val), dtype=torch.long)\n",
        "batch_size = 64\n",
        "del X_norm, X_val_norm"
      ],
      "metadata": {
        "id": "5HtLRwe3RN5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = X_train_tensor.shape[1] # 78\n",
        "rbm = TRBM(input_size, 50, device)\n",
        "rbm = rbm.to(device)\n",
        "print(datetime.datetime.now())\n",
        "print(\"Training RBM using the original RBM code...\")\n",
        "rbm.fit(X_train_tensor, iterations=20, learning_rate=0.01, cd_n=1, batch_size=256, verbose=True)\n",
        "print(\"RBM training complete.\")\n",
        "print(datetime.datetime.now())"
      ],
      "metadata": {
        "id": "5XKHygLERQcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}